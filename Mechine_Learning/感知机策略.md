**力求以通俗易懂的方式将感知机解释明白**



[TOC]

### 1.数据集的线形可分性

假设有这么一个数据集
$$
T=\{(x_1,y_1),(x_2,y_2),···(x_n,y_n)\}
$$

其中，x~i~$\in$ R^n^ ，y~i~$\in${-1,+1}


感知机的作用就是找到一个超平面$S$
$$
w·x +b=0
$$

能够将数据集的正实例（也就是+1）和负实例分开。对所有$y_2=+1$的实例i，有$w·x_i+b>0$

对所有$y_i=-1$的实例i，有$w·x_i+b<0$  。

如果能够将所有的实例分开，那就称数据集$T$线性可分，否则就不可分



### 2.感知机学习策略

感知机就是要找到这么一个超平面，将正负实例分开，则就需要确定超平面的w，b。这里我们引进损失函数的概念，损失函数的一个想当然的选择就是误分类点的个数，我们总期望是误分类点的个数最少，即
$$
Min\{count(误分类点)\}
$$
可由于个数并不是$w,b$的一个**连续可导**函数（要连续可导是为了后面用偏导的方式求得最大下降参数），那么我们选择求误分类点到平面$S$的总距离，使其总距离最小。

那么输入空间$R^n$中任意一点$x_0$到超平面的距离$S$为：
$$
\frac{1}{||w||}|w·x_0+b|
$$
$||w||$是w的L~2~范数，就是通常意义上的膜

==关于点到空间中超平面的距离推导，请看我另一篇文章==

那么对于误分类的数据$(x_i,y_i)$，有
$$
-y_i(w·x+b)>0
$$
因为$w·x_i+b>0$（误分类，正确分类为$w·x_i+b<0,y_i=-1$）,反之依然

那么误分类点到超平面的距离为
$$
-\frac{1}{||w||}y_i(w·x_i+b)
$$
那么空间所有点的合集到超平面$S$的合集为
$$
-\frac{1}{||w||}\sum_{x_i\in M}{}y_i(w·x_i+b)
$$
不考虑$\frac{1}{||w||}$,就能得到损失函数了～

损失函数定义为：
$$
L(w,b)=-\sum_{x_i\in M}{}y_i(w·x_i+b)
$$
可以很明显的看出，损失函数非负，若没有损失点，损失函数值为0，我们的目的就是使其最小
$$
\min_{w,b}L(w,b)=-\sum_{x_i\in M}{}y_i(w·x_i+b)
$$
我们采用梯度下降算法，随机选择一个超平面$w_0,b_0$用梯度下降算法不断的极小化目标函数$(9)$

在$w$参数的梯度(对$w$求偏导即可)为


$$
\nabla_wL(w,b) = -\sum_{x_i\in M}y_ix_i
$$
在$b$参数的梯度为
$$
\nabla_bL(w,b) = -\sum_{x_i\in M}y_i
$$
**随机选择一个误分类点，对$w,b$进行更新**
$$
w\gets w+\eta y_ix_i
$$

$$
b\gets b+\eta y_i
$$

其中，$\eta$为步长，又称学习率，可以迭代可以使$L(w,b)$不断减小。





【完】



