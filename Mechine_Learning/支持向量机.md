[TOC]

### 1.1线性可分支持向量机

我们知道，当训练数据密集可分时，存在无穷个分离超平面可将两类数据正确的分开，感知机是利用误分类最小的原则，求得出分离超平面，不过这样有一个问题，这时的解会有无穷多个。而线性可分支持向量机利用间隔最大的原则，求出最优的分离超平面。这样，解就是唯一的了。



### 1.2 函数间隔与几何间隔

一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$w·x+b=0$确定的情况下，$|w·x+b|$能够相对的表示点$x$距离超平面的远近，而$w·x+b$的符号与类标记$y$的符号能否一致能够表示分类的是否正确，所以用$y(w·x+b)$来表示分类的正确性及确信度，这就是函数间隔。

虽然函数间隔可以表示分类预测的正确性及确信度。但是选择分离平面时，只有函数间隔是不够的。因为只要成比例的改变$w$和$q$，函数间隔也会成比例的改变，这种情景我们可以对分离的超平面的法向量$w$加约束，比如$||w||=1$，这样使得间隔确定，这时候间隔就变成了几何间隔。


$$
\Upsilon =  \frac{\hat{\Upsilon}}{||w||}
$$
其中，$\Upsilon$为几何间隔，$\hat{\Upsilon}$为函数间隔



### 1.3 间隔最大化

支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对线性可分的训练数据集而言，线性可分分离超平面有无情多个（感知机），但是几何间隔最大的分离超平面是唯一的。这种间隔最大化又称为硬间隔最大化。

间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。就是说，不仅将正负实例点分开，而且对最难分的实例点也有足够的确信度降它们分开。

间隔最大化可以表示为如下的约束最优化问题：


$$
\max\limits_{w,b}    \quad    \Upsilon
$$

$$
s.t. \quad y_i(\frac{w}{||w||}·x_i+\frac{b}{||w||})\ge \Upsilon,\quad i=1,2,3,···,N
$$

即我们希望最大化超平面$(w,b)$关于训练数据集的几何间隔$\Upsilon$，约束条件表示的是超平面$(w,b)$关于每个训练样本的几何间隔至少是$\Upsilon$.

由(1)式，我们可以得到


$$
\max\limits_{w,b}    \quad    \frac{\hat{\Upsilon}}{||w||}
$$

$$
s.t. \quad y_i({w}·x_i+{b})\ge \hat{\Upsilon},\quad i=1,2,3,···,N
$$

函数间隔$\hat{\Upsilon}$的取值并不影响最优化问题解。事实上，假设将$w$和$b$按比例改变$\lambda w$和$\lambda b$，这时函数间隔成为$\lambda\hat{\Upsilon}$。函数间隔这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，他产生了一个等价的最优化问题。这样，就可以取$\hat{\Upsilon}=1$。将$\hat{\Upsilon}=1$带入上面的最优化问题，将最大化$\frac{1}{||w||}$等价变化为最小化$\frac{1}{2}||w||^2$是等价的，于是我们得到
$$
\min\limits_{w,b}    \quad    \frac{1}{2}||w||^2
$$

$$
s.t. \quad y_i({w}·x_i+{b})-1\ge 0,\quad i=1,2,3,···,N
$$
那么我们最大间隔法即为

（1）构造并求解约束最优化问题：
$$
\min\limits_{w,b}    \quad    \frac{1}{2}||w||^2
$$

$$
s.t. \quad y_i({w}·x_i+{b})-1\ge 0,\quad i=1,2,3,···,N
$$

求得最优解$w^*,b^*$

（2）由此得到分离超平面：
$$
w^*·x+b=0
$$
分类决策函数为
$$
f(x)=sign(w^*·x+b^*)
$$
