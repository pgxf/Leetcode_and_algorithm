

                         						# 机器学习课程论文

​																		**学号：1701210685**									

​																		**姓名：邢峰**



前言：

推导了几个常用机器学习的算法



[TOC]



## 一.感知机

### 1.1数据集的线形可分性

假设有这么一个数据集
$$
T=\{(x_1,y_1),(x_2,y_2),···(x_n,y_n)\}
$$
其中，x~i~$\in$ R^n^ ，y~i~$\in${-1,+1}

感知机的作用就是找到一个超平面$S$
$$
w·x +b=0
$$
能够将数据集的正实例（也就是+1）和负实例分开。对所有$y_2=+1$的实例i，有$w·x_i+b>0$

对所有$y_i=-1$的实例i，有$w·x_i+b<0$  。

如果能够将所有的实例分开，那就称数据集$T$线性可分，否则就不可分



### 1.2感知机学习策略

感知机就是要找到这么一个超平面，将正负实例分开，则就需要确定超平面的w，b。这里我们引进损失函数的概念，损失函数的一个想当然的选择就是误分类点的个数，我们总期望是误分类点的个数最少，即
$$
Min\{count(误分类点)\}
$$
可由于个数并不是$w,b$的一个**连续可导**函数（要连续可导是为了后面用偏导的方式求得最大下降参数），那么我们选择求误分类点到平面$S$的总距离，使其总距离最小。

那么输入空间$R^n$中任意一点$x_0$到超平面的距离$S$为：
$$
\frac{1}{||w||}|w·x_0+b|
$$
$||w||$是w的L~2~范数，就是通常意义上的膜

==关于点到空间中超平面的距离推导，请看我另一篇文章==

那么对于误分类的数据$(x_i,y_i)$，有
$$
-y_i(w·x+b)>0
$$
因为$w·x_i+b>0$（误分类，正确分类为$w·x_i+b<0,y_i=-1$）,反之依然

那么误分类点到超平面的距离为
$$
-\frac{1}{||w||}y_i(w·x_i+b)
$$
那么空间所有点的合集到超平面$S$的合集为
$$
-\frac{1}{||w||}\sum_{x_i\in M}{}y_i(w·x_i+b)
$$
不考虑$\frac{1}{||w||}$,就能得到损失函数了～

损失函数定义为：
$$
L(w,b)=-\sum_{x_i\in M}{}y_i(w·x_i+b)
$$
可以很明显的看出，损失函数非负，若没有损失点，损失函数值为0，我们的目的就是使其最小
$$
\min_{w,b}L(w,b)=-\sum_{x_i\in M}{}y_i(w·x_i+b)
$$
我们采用梯度下降算法，随机选择一个超平面$w_0,b_0$用梯度下降算法不断的极小化目标函数$(9)$

在$w$参数的梯度(对$w$求偏导即可)为
$$
\nabla_wL(w,b) = -\sum_{x_i\in M}y_ix_i
$$
在$b$参数的梯度为
$$
\nabla_bL(w,b) = -\sum_{x_i\in M}y_i
$$
**随机选择一个误分类点，对$w,b$进行更新**
$$
w\gets w+\eta y_ix_i
$$

$$
b\gets b+\eta y_i
$$

其中，$\eta$为步长，又称学习率，可以迭代可以使$L(w,b)$不断减小。





## 二.朴素贝叶斯



朴素贝叶斯基于的是贝叶斯定理与特征条件独立假设的分类方法。



==朴素贝叶斯法对条件概率分布作了条件独立性的假设==
$$
P(X=x|Y=c_k)  = P(X^{(1)}=x^{(1)},···，X^{(n)}=x^{(n)}|Y=c_k)=\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)
$$
上式的意思是说假设X代表好瓜，属性x^(1)^	为色泽，x^(2)^	为形状，等，他们与彼此不产生任何关系。



朴素贝叶斯进行分类时，对给定的输入x，通过学习到的模型计算后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为$x$类的输出，后验概率计算根据贝叶斯定理进行：
$$
P(Y=x_k|X=x) = \frac{P(X=x|Y=c_k) P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}
$$
则将（14）带入（15），有
$$
P(Y=x_k|X=x)  = \frac{P(Y=c_k)\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}
$$
这就是朴素贝叶斯的基本公式。

则朴素贝叶斯分类器可表示为
$$
y=f(x)=arg max_{c_k}\frac{P(Y=c_k)\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}
$$
其中，分母对所有分分类c~k~来说都是相同的，故
$$
y=f(x)=arg max_{c_k}P(Y=c_k)\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)
$$




## 三.支持向量机



### 3.1线性可分支持向量机

我们知道，当训练数据密集可分时，存在无穷个分离超平面可将两类数据正确的分开，感知机是利用误分类最小的原则，求得出分离超平面，不过这样有一个问题，这时的解会有无穷多个。而线性可分支持向量机利用间隔最大的原则，求出最优的分离超平面。这样，解就是唯一的了。



### 3.2 函数间隔与几何间隔

一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$w·x+b=0$确定的情况下，$|w·x+b|$能够相对的表示点$x$距离超平面的远近，而$w·x+b$的符号与类标记$y$的符号能否一致能够表示分类的是否正确，所以用$y(w·x+b)$来表示分类的正确性及确信度，这就是函数间隔。

虽然函数间隔可以表示分类预测的正确性及确信度。但是选择分离平面时，只有函数间隔是不够的。因为只要成比例的改变$w$和$q$，函数间隔也会成比例的改变，这种情景我们可以对分离的超平面的法向量$w$加约束，比如$||w||=1$，这样使得间隔确定，这时候间隔就变成了几何间隔。


$$
\Upsilon =  \frac{\hat{\Upsilon}}{||w||}
$$
其中，$\Upsilon$为几何间隔，$\hat{\Upsilon}$为函数间隔



### 3.3 间隔最大化

支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对线性可分的训练数据集而言，线性可分分离超平面有无情多个（感知机），但是几何间隔最大的分离超平面是唯一的。这种间隔最大化又称为硬间隔最大化。

间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。就是说，不仅将正负实例点分开，而且对最难分的实例点也有足够的确信度降它们分开。

间隔最大化可以表示为如下的约束最优化问题：


$$
\max\limits_{w,b}    \quad    \Upsilon
$$

$$
s.t. \quad y_i(\frac{w}{||w||}·x_i+\frac{b}{||w||})\ge \Upsilon,\quad i=1,2,3,···,N
$$

即我们希望最大化超平面$(w,b)$关于训练数据集的几何间隔$\Upsilon$，约束条件表示的是超平面$(w,b)$关于每个训练样本的几何间隔至少是$\Upsilon$.

由(19)式，我们可以得到


$$
\max\limits_{w,b}    \quad    \frac{\hat{\Upsilon}}{||w||}
$$

$$
s.t. \quad y_i({w}·x_i+{b})\ge \hat{\Upsilon},\quad i=1,2,3,···,N
$$

函数间隔$\hat{\Upsilon}$的取值并不影响最优化问题解。事实上，假设将$w$和$b$按比例改变$\lambda w$和$\lambda b$，这时函数间隔成为$\lambda\hat{\Upsilon}$。函数间隔这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，他产生了一个等价的最优化问题。这样，就可以取$\hat{\Upsilon}=1$。将$\hat{\Upsilon}=1$带入上面的最优化问题，将最大化$\frac{1}{||w||}$等价变化为最小化$\frac{1}{2}||w||^2$是等价的，于是我们得到
$$
\min\limits_{w,b}    \quad    \frac{1}{2}||w||^2
$$

$$
s.t. \quad y_i({w}·x_i+{b})-1\ge 0,\quad i=1,2,3,···,N
$$
那么我们最大间隔法即为

（1）构造并求解约束最优化问题：
$$
\min\limits_{w,b}    \quad    \frac{1}{2}||w||^2
$$

$$
s.t. \quad y_i({w}·x_i+{b})-1\ge 0,\quad i=1,2,3,···,N
$$

求得最优解$w^*,b^*$

（2）由此得到分离超平面：
$$
w^*·x+b=0
$$
分类决策函数为
$$
f(x)=sign(w^*·x+b^*)
$$




## 4.神经网络



###4.1前馈神经网络

将多个神经元逐层互联形成的网络结构，得到前馈神经网络模型，也称多层感知机。

如下图中，第一层叫做输入层，最后一层叫做输出层，中间的层叫做隐藏层。

如下还是一个全连接层，即每个节点都与下层所有节点有连接。



![屏幕快照 2017-12-30 15.00.22](../../../Desktop/屏幕快照 2017-12-30 15.00.22.png)

可以很自然的得到如下列式


$$
z_1^2 = w_{11}^{(2)}x_1+ w_{12}^{(2)}x_2+ w_{13}^{(2)}x_3+ b_{1}^{(2)}
$$

$$
z_2^2 = w_{21}^{(2)}x_1+ w_{22}^{(2)}x_2+ w_{23}^{(2)}x_3+ b_{2}^{(2)}
$$

$$
z_3^{(2)} = w_{31}^{(2)}x_1+ w_{32}^{(2)}x_2+ w_{33}^{(2)}x_3+ b_{3}^{(2)}
$$

则
$$
\textbf{z} =\begin{equation}  
  \left(  
  \begin{array}{c}  
          z_{1}^2 \\         
          z_{2}^2 \\
          z_3^2
 \end{array}  
 \right)  
\end{equation}  
    =\textbf{W}\textbf{X}=
    \left(  
  \begin{array}{ccc}  
          {w}_{11}^{(2)} &w_{12}^{(2)} & {w}_{13}^{(2)}\\  
          {w}_{21}^{(2)} &w_{22}^{(2)} & {w}_{23}^{(2)}\\  
          {w}_{31}^{(2)} &w_{32}^{(2)} & {w}_{33}^{(2)}\\  
          {b}_{1}^{(2)} &b_{2}^{(2)} & {b}_{3}^{(2)}\\  
  \end{array}  
  \right)
   \left(  
  \begin{array}{c}  
          x_{1} \\         
          x_{2} \\
          x_3  \\
          1
 \end{array}  
 \right)
$$
我们通过这样叠加的方式，最终得到的数据还是线性的，那么我们需要每次在神经元进行非线性话，就有了激活函数，一般的激活函数有Sigmoid、Hyperbolic tangent、hand tanh、Rectified Linear Unit

Sigmoid为
$$
\sigma (x) = \frac{1}{1+e^{-x}}
$$
Hyperbolic tangent为
$$
tanh(x) = \frac{e^{2x}-1}{e^{2x}+1}
$$
关于如何选取激活函数，没有确定的方法，需要实验。

若要对输出使其支持最大似然估计，那么需要对输出层进行归一
$$
\textbf{y} = y_1y_2···y_k
$$

$$
softmax(y_i)=\frac{e^{y_i}}{\sum_{j=1}^{k}e^{y_j}}
$$

损失函数就不提了，还是那个原则，训练目标，使其损失函数最小
$$
\hat{\Theta} = argminL(\hat{y,y})
$$

$$
\Theta = (W^1,W^2,...,b^1,b^2,...,E)
$$

现在我们需要训练我们的神经网络，用梯度下降法，最速下降方向是负梯度方向$-\hat{g}$

更新原则为，其中，$\eta$为学习率
$$
\Theta \gets\Theta -\eta\hat{g}
$$

### 4.2卷积神经网络

### 4.3循环神经网络





## 五.结语

由于时间有限，只能从[自己的博客](https://blog.csdn.net/better_xf)搬下进行了修改，底层算法实现没有进行详细的书写。





## 六.参考文献

李航.统计学习方法[M].北京：清华大学出版社，2012：25-128.