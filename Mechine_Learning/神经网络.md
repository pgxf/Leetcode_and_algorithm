[TOC]

### 1.前馈神经网络

将多个神经元逐层互联形成的网络结构，得到前馈神经网络模型，也称多层感知机。

如下图中，第一层叫做输入层，最后一层叫做输出层，中间的层叫做隐藏层。

如下还是一个全连接层，即每个节点都与下层所有节点有连接。



![屏幕快照 2017-12-30 15.00.22](../../../Desktop/屏幕快照 2017-12-30 15.00.22.png)

可以很自然的得到如下列式


$$
z_1^2 = w_{11}^{(2)}x_1+ w_{12}^{(2)}x_2+ w_{13}^{(2)}x_3+ b_{1}^{(2)}
$$

$$
z_2^2 = w_{21}^{(2)}x_1+ w_{22}^{(2)}x_2+ w_{23}^{(2)}x_3+ b_{2}^{(2)}
$$

$$
z_3^{(2)} = w_{31}^{(2)}x_1+ w_{32}^{(2)}x_2+ w_{33}^{(2)}x_3+ b_{3}^{(2)}
$$

则
$$
\textbf{z} =\begin{equation}  
  \left(  
  \begin{array}{c}  
          z_{1}^2 \\         
          z_{2}^2 \\
          z_3^2
 \end{array}  
 \right)  
\end{equation}  
    =\textbf{W}\textbf{X}=
    \left(  
  \begin{array}{ccc}  
          {w}_{11}^{(2)} &w_{12}^{(2)} & {w}_{13}^{(2)}\\  
          {w}_{21}^{(2)} &w_{22}^{(2)} & {w}_{23}^{(2)}\\  
          {w}_{31}^{(2)} &w_{32}^{(2)} & {w}_{33}^{(2)}\\  
          {b}_{1}^{(2)} &b_{2}^{(2)} & {b}_{3}^{(2)}\\  
  \end{array}  
  \right)
   \left(  
  \begin{array}{c}  
          x_{1} \\         
          x_{2} \\
          x_3  \\
          1
 \end{array}  
 \right)
$$
我们通过这样叠加的方式，最终得到的数据还是线性的，那么我们需要每次在神经元进行非线性话，就有了激活函数，一般的激活函数有Sigmoid、Hyperbolic tangent、hand tanh、Rectified Linear Unit

Sigmoid为
$$
\sigma (x) = \frac{1}{1+e^{-x}}
$$
Hyperbolic tangent为
$$
tanh(x) = \frac{e^{2x}-1}{e^{2x}+1}
$$
关于如何选取激活函数，没有确定的方法，需要实验。

若要对输出使其支持最大似然估计，那么需要对输出层进行归一
$$
\textbf{y} = y_1y_2···y_k
$$

$$
softmax(y_i)=\frac{e^{y_i}}{\sum_{j=1}^{k}e^{y_j}}
$$

损失函数就不提了，还是那个原则，训练目标，使其损失函数最小
$$
\hat{\Theta} = argminL(\hat{y,y})
$$

$$
\Theta = (W^1,W^2,...,b^1,b^2,...,E)
$$

现在我们需要训练我们的神经网络，用梯度下降法，最速下降方向是负梯度方向$-\hat{g}$

更新原则为，其中，$\eta$为学习率
$$
\Theta \gets\Theta -\eta\hat{g}
$$

### 2.卷积神经网络

### 3.循环神经网络



